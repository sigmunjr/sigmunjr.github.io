<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Ockhams barberkniv er en bærebjelke for store språkmodeller | Sigmund J. G. Rolfsjord </title> <meta name="author" content="Sigmund J. G. Rolfsjord"> <meta name="description" content="I vitenskapen har det lenge vært en grunnregel at de enkleste forklaringene er de beste, dette er også mekanismen som gjør at transformer modellene vi finner i ChatGPT fungerer."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sigmunjr.github.io/blog/2025/ockhams_spraakmodeller/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Sigmund</span> J. G. Rolfsjord </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">om meg </a> </li> <li class="nav-item active"> <a class="nav-link" href="/index.html">blogg </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Ockhams barberkniv er en bærebjelke for store språkmodeller</h1> <p class="post-meta"> Created on March 26, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/transformere"> <i class="fa-solid fa-hashtag fa-sm"></i> transformere</a>   <a href="/blog/tag/ki"> <i class="fa-solid fa-hashtag fa-sm"></i> KI</a>   ·   <a href="/blog/category/teori"> <i class="fa-solid fa-tag fa-sm"></i> teori</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <p> I vitenskapen har det lenge vært en grunnregel at de enkleste forklaringene er de beste. Når en teori kan forklare noe med færrest mulig antakelser, regnes den som mer elegant – ja, noen ganger til og med vakker. Dette prinsippet kalles Ockhams barberkniv: alt som er unødvendig bør skjæres vekk. </p> <p> Selv Rema 1000 har fanget opp ideen med slagordet “Det enkle er ofte det beste”. Og kanskje er det ikke så overraskende at dette prinsippet – forenkling – også ligger til grunn for hvordan moderne kunstig intelligens fungerer? </p> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ockham_kutter-480.webp 480w,/assets/img/blog/ockham_kutter-800.webp 800w,/assets/img/blog/ockham_kutter-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog/ockham_kutter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Ockham er ute og forenkler med barberkniven sin (AI-generert chatgpt 4o) </div> </div> </div> <h2 id="hva-er-egentlig-en-transformer">Hva er egentlig en transformer?</h2> <p>Transformere er en type maskinlæringsmodeller som ligger bak de store språkmodellene – som ChatGPT, Gemini og Claude. Disse modellene er laget for å forstå sammenhengen mellom ord, og forutsi hva som kommer videre i en tekst.</p> <p>Det som gjør transformere unike, er det vi kaller <strong>oppmerksomhetsmekanismer</strong>. Disse lar modellen vurdere hvilke ord i en tekst som er viktige i sammenhengen. I stedet for å behandle alle ord likt, fokuserer modellen på utvalgte deler av teksten, og gir disse mer “vekt”. s du bytter ut et uvesentlig ord i en setning, vil svaret som oftest være det samme – fordi mod Dette skjer blant annet gjennom matematiske funksjoner som <strong>softmax</strong> og <strong>ReLU</strong>, som bidrar til å forsterke signalene fra enkelte ord og dempe andre. På den måten lærer modellen å hente ut det vesentlige – og overse støy.</p> <h2 id="hvorfor-velger-modellen-det-enkle">Hvorfor velger modellen det enkle?</h2> <p>Språkmodeller har det som kalles en <strong>forenklingstendens</strong> (simplicity bias). Det betyr at de har en naturlig tilbøyelighet til å velge de enkleste løsningene først – altså sammenhenger mellom få og nærliggende ord.</p> <p>Dette gjør dem mer robuste. Hvis du bytter ut et uvesentlig ord i en setning, vil svaret som oftest være det samme – fordi modellen har lært å fokusere på det som faktisk betyr noe.</p> <p>Under trening begynner modeller ofte med å lære grunnleggende mønstre, som hvordan subjekt og verb henger sammen, eller hvordan kjønn og entall/flertall uttrykkes i språket. Først etter at denne basiskunnskapen er på plass, beveger modellen seg videre til mer komplekse oppgaver, som logiske resonnementer eller tekstforståelse på høyt nivå.</p> <h2 id="fra-enkle-mønstre-til-kompleks-forståelse">Fra enkle mønstre til kompleks forståelse</h2> <p>Selv om modellene starter med det enkle, er de ikke begrenset til det. Transformere er i stand til å finne svært kompliserte mønstre – spesielt når de får nok dybde og kontekst.</p> <p><strong>Kontekstlengde</strong> handler om hvor mye tekst modellen kan ta inn samtidig. Store modeller som Gemini 2.5 kan analysere opptil 700 000 ord på én gang. Det gir enorm kapasitet – men også et behov for å velge ut den informasjonen som faktisk er relevant.</p> <p><strong>Dybden</strong> i en modell viser hvor mange lag med analyse den gjør før den gir et svar. Hvert lag kan kombinere informasjon på nye måter. DeepSeek V3, for eksempel, har 61 slike lag. Når hvert ord i en tekst kan kobles til alle andre ord, og dette skjer i lag etter lag, blir det mulig å oppdage svært komplekse sammenhenger – selv om modellen begynner med det enkle.</p> <h2 id="ockhams-barberkniv-i-praksis">Ockhams barberkniv i praksis</h2> <p>Ockhams barberkniv handler om mer enn filosofi – den viser seg å være en nøkkel til hvorfor språkmodeller faktisk fungerer. Forskere som Rende og Bhattamishra peker på nettopp denne evnen til å finne enkle forklaringer som en viktig grunn til modellenes suksess.</p> <p>Transformer-modeller viser seg nyttige på mange områder hvor man skal modellere svært kompliserte sammenhenger, som å gjette “utseende” til et protein basert på RNA-sekvenser (AlphaFold), eller fysikksimuleringer.</p> <p>Det som kjennetegner disse problemene, hvor transformerne gjør det veldig bra, er data der det er ufattelig mange mulige sammenhenger, men hvor de faktiske sammenhengene er enklere. Disse problemene er det mange av. Kanskje du kjenner til et mulig neste problem?</p> </div> </article> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>